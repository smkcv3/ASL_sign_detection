{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a36d1e52-7699-4bd4-8266-b03465931817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 13:59:58.740 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\iamsm\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ New trained models loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 14:00:02.944 Session state does not function when running a script without `streamlit run`\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "st.session_state has no attribute \"camera_running\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\streamlit\\runtime\\state\\session_state.py:398\u001b[0m, in \u001b[0;36mSessionState.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem(widget_id, key)\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\streamlit\\runtime\\state\\session_state.py:443\u001b[0m, in \u001b[0;36mSessionState._getitem\u001b[1;34m(self, widget_id, user_key)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;66;03m# We'll never get here\u001b[39;00m\n\u001b[1;32m--> 443\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\streamlit\\runtime\\state\\session_state_proxy.py:119\u001b[0m, in \u001b[0;36mSessionStateProxy.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\streamlit\\runtime\\state\\session_state_proxy.py:90\u001b[0m, in \u001b[0;36mSessionStateProxy.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     89\u001b[0m require_valid_user_key(key)\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_session_state()[key]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\streamlit\\runtime\\state\\safe_session_state.py:91\u001b[0m, in \u001b[0;36mSafeSessionState.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state[key]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\streamlit\\runtime\\state\\session_state.py:400\u001b[0m, in \u001b[0;36mSessionState.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(_missing_key_error_message(key))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'st.session_state has no key \"camera_running\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 184\u001b[0m\n\u001b[0;32m    181\u001b[0m     st\u001b[38;5;241m.\u001b[39msession_state\u001b[38;5;241m.\u001b[39mcamera_running \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# Main camera loop\u001b[39;00m\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m st\u001b[38;5;241m.\u001b[39msession_state\u001b[38;5;241m.\u001b[39mcamera_running:\n\u001b[0;32m    185\u001b[0m     \n\u001b[0;32m    186\u001b[0m     \u001b[38;5;66;03m# Initialize webcam\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\streamlit\\runtime\\state\\session_state_proxy.py:121\u001b[0m, in \u001b[0;36mSessionStateProxy.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(_missing_attr_error_message(key))\n",
      "\u001b[1;31mAttributeError\u001b[0m: st.session_state has no attribute \"camera_running\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# üìÅ CONFIGURATION\n",
    "IMG_SIZE = 224  # Same as training pipeline\n",
    "CONFIDENCE_THRESHOLD = 0.8  # Lower than before since we expect better performance\n",
    "\n",
    "# üî≤ ROI COORDINATES - EXACT MATCH with data collection\n",
    "ROI_top = 80        # Same as data_capture.py\n",
    "ROI_bottom = 320    # Same as data_capture.py  \n",
    "ROI_left = 130      # Same as data_capture.py\n",
    "ROI_right = 370     # Same as data_capture.py\n",
    "\n",
    "# Define word dictionary\n",
    "word_dict = {0:'Zero', 1:'One', 2:'Two', 3:'Three', 4:'Four', 5:'Five', 6:'Six', 7:'Seven', 8:'Eight', 9:'Nine'}\n",
    "\n",
    "# üöÄ LOAD NEW TRAINED MODELS\n",
    "@st.cache_resource\n",
    "def load_models():\n",
    "    \"\"\"Load the new trained models with caching\"\"\"\n",
    "    try:\n",
    "        # Try to load new models first\n",
    "        custom_cnn = load_model('new_custom_cnn.h5')\n",
    "        mobilenet = load_model('new_mobilenet.h5') \n",
    "        print(\"‚úÖ New trained models loaded successfully!\")\n",
    "        return custom_cnn, mobilenet, \"NEW\"\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            # Fallback to old models if new ones don't exist yet\n",
    "            custom_cnn = load_model('custom_cnn.h5')\n",
    "            mobilenet = load_model('custom_mobilenet.h5')\n",
    "            print(\"‚ö†Ô∏è Using old models - train new models first!\")\n",
    "            return custom_cnn, mobilenet, \"OLD\"\n",
    "        except Exception as e2:\n",
    "            st.error(f\"‚ùå Error loading NEW models: {str(e)}\")\n",
    "            st.error(f\"‚ùå Error loading OLD models: {str(e2)}\")\n",
    "            st.error(\"üí° Make sure these files exist in your project folder:\")\n",
    "            st.code(\"new_custom_cnn.h5\\ncustom_mobilenet.h5\")\n",
    "            st.info(\"üìã Run ASLp_new.ipynb to create these model files\")\n",
    "            st.stop()\n",
    "\n",
    "# Load models with detailed feedback\n",
    "with st.spinner('Loading models...'):\n",
    "    custom_cnn, mobilenet, model_version = load_models()\n",
    "    \n",
    "# Display model information\n",
    "if model_version == \"NEW\":\n",
    "    st.success(\"üî• New trained models loaded successfully!\")\n",
    "    st.info(\"üéØ Optimized for your webcam data - expect high performance!\")\n",
    "    \n",
    "    # Model verification\n",
    "    try:\n",
    "        # Test models with dummy data\n",
    "        test_input = np.random.random((1, IMG_SIZE, IMG_SIZE, 3)).astype(np.float32)\n",
    "        _ = custom_cnn.predict(test_input, verbose=0)\n",
    "        _ = mobilenet.predict(test_input, verbose=0)\n",
    "        st.success(\"‚úÖ Model functionality verified!\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"‚ö†Ô∏è Model verification failed: {str(e)}\")\n",
    "else:\n",
    "    st.warning(\"‚ö†Ô∏è Using old models - performance may be limited\")\n",
    "    st.info(\"üí° Train new models with ASLp_new.ipynb for best results\")\n",
    "\n",
    "def preprocess_roi_simple(frame, ROI_top, ROI_bottom, ROI_left, ROI_right):\n",
    "    \"\"\"\n",
    "    üî• EXACT MATCH preprocessing pipeline:\n",
    "    Data Collection: 240x240 BGR raw ‚Üí Training: resize+normalize+RGB ‚Üí Inference: same\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Extract ROI - EXACT SAME coordinates as data_capture.py\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_left:ROI_right]  # 240x240 BGR\n",
    "    \n",
    "    # 2. Resize to training size - SAME as ImageDataGenerator target_size\n",
    "    roi_resized = cv2.resize(roi, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_LANCZOS4)  # 224x224 BGR\n",
    "    \n",
    "    # 3. Convert BGR‚ÜíRGB - SAME as ImageDataGenerator automatic conversion\n",
    "    roi_rgb = cv2.cvtColor(roi_resized, cv2.COLOR_BGR2RGB)  # 224x224 RGB\n",
    "    \n",
    "    # 4. Normalize [0,255]‚Üí[0,1] - SAME as ImageDataGenerator rescale=1./255\n",
    "    roi_normalized = roi_rgb.astype(np.float32) / 255.0  # [0,1] range\n",
    "    \n",
    "    # 5. Add batch dimension - SAME as training expects\n",
    "    roi_batch = np.expand_dims(roi_normalized, axis=0)  # (1, 224, 224, 3)\n",
    "    \n",
    "    return roi_batch, roi_resized\n",
    "\n",
    "def simple_hand_detection(roi):\n",
    "    \"\"\"\n",
    "    Simple hand detection based on content analysis\n",
    "    Much faster than complex segmentation\n",
    "    \"\"\"\n",
    "    # Convert to grayscale for analysis\n",
    "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Check for sufficient contrast (hand vs background)\n",
    "    contrast = np.std(gray)\n",
    "    \n",
    "    # Check for non-uniform regions (indicating hand presence)\n",
    "    mean_intensity = np.mean(gray)\n",
    "    \n",
    "    # Simple thresholds based on typical hand characteristics\n",
    "    has_hand = (contrast > 15 and 50 < mean_intensity < 200)\n",
    "    \n",
    "    # Calculate a simple confidence score\n",
    "    hand_confidence = min(contrast / 30.0, 1.0) if has_hand else 0.0\n",
    "    \n",
    "    return has_hand, hand_confidence\n",
    "\n",
    "def create_ensemble_prediction(roi_batch):\n",
    "    \"\"\"Create ensemble prediction from both models\"\"\"\n",
    "    \n",
    "    # Get predictions from both models\n",
    "    pred_cnn = custom_cnn.predict(roi_batch, verbose=0)\n",
    "    pred_mobilenet = mobilenet.predict(roi_batch, verbose=0)\n",
    "    \n",
    "    # Simple ensemble: average predictions\n",
    "    ensemble_pred = (pred_cnn + pred_mobilenet) / 2.0\n",
    "    \n",
    "    # Get prediction details\n",
    "    predicted_class = np.argmax(ensemble_pred)\n",
    "    confidence = np.max(ensemble_pred)\n",
    "    predicted_gesture = word_dict[predicted_class]\n",
    "    \n",
    "    # Get all class probabilities for analysis\n",
    "    class_probs = ensemble_pred[0]\n",
    "    \n",
    "    return predicted_gesture, confidence, class_probs, predicted_class\n",
    "\n",
    "# Streamlit app layout\n",
    "st.title(\"üéØ ASL Hand Gesture Recognition - Enhanced\")\n",
    "if model_version == \"NEW\":\n",
    "    st.write(\"üî• **Using NEW trained models** (optimized for webcam data)\")\n",
    "    st.success(\"‚úÖ Models trained on your webcam conditions\")\n",
    "else:\n",
    "    st.write(\"‚ö†Ô∏è **Using OLD models** - Please train new models first!\")\n",
    "    st.warning(\"üîÑ Run ASLp_new.ipynb to train optimized models\")\n",
    "\n",
    "st.write(\"Real-time ASL digit recognition (0-9) with ensemble deep learning models\")\n",
    "\n",
    "# Configuration display with pipeline verification\n",
    "with st.expander(\"üîß System Configuration & Pipeline Verification\"):\n",
    "    st.write(f\"**ROI Coordinates:** Top:{ROI_top}, Bottom:{ROI_bottom}, Left:{ROI_left}, Right:{ROI_right}\")\n",
    "    st.write(f\"**ROI Size:** {ROI_right-ROI_left} √ó {ROI_bottom-ROI_top} pixels (same as data collection)\")\n",
    "    st.write(f\"**Processing Size:** {IMG_SIZE} √ó {IMG_SIZE} pixels (same as training)\")\n",
    "    st.write(f\"**Confidence Threshold:** {CONFIDENCE_THRESHOLD}\")\n",
    "    st.write(f\"**Model Version:** {model_version}\")\n",
    "    \n",
    "    # Pipeline verification\n",
    "    st.markdown(\"**üîÑ Processing Pipeline Verification:**\")\n",
    "    st.write(\"‚úÖ Data Collection: 240√ó240 BGR raw images\")\n",
    "    st.write(\"‚úÖ Training: 240√ó240 ‚Üí 224√ó224, BGR‚ÜíRGB, normalize [0,1]\")\n",
    "    st.write(\"‚úÖ Inference: 240√ó240 ‚Üí 224√ó224, BGR‚ÜíRGB, normalize [0,1]\")\n",
    "    st.success(\"‚úÖ Perfect pipeline alignment achieved!\")\n",
    "\n",
    "# Placeholders for webcam feed and prediction\n",
    "FRAME_WINDOW = st.image([])\n",
    "PREDICTION_TEXT = st.empty()\n",
    "METRICS_TEXT = st.empty()\n",
    "\n",
    "# Control buttons\n",
    "col1, col2 = st.columns(2)\n",
    "with col1:\n",
    "    start_button = st.button(\"üöÄ Start Camera\", type=\"primary\")\n",
    "with col2:\n",
    "    stop_button = st.button(\"‚èπÔ∏è Stop\", type=\"secondary\")\n",
    "\n",
    "# Initialize session state\n",
    "if 'camera_running' not in st.session_state:\n",
    "    st.session_state.camera_running = False\n",
    "\n",
    "# Camera control\n",
    "if start_button:\n",
    "    st.session_state.camera_running = True\n",
    "    \n",
    "if stop_button:\n",
    "    st.session_state.camera_running = False\n",
    "\n",
    "# Main camera loop\n",
    "if st.session_state.camera_running:\n",
    "    \n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        st.error(\"‚ùå Error: Could not open webcam\")\n",
    "        st.stop()\n",
    "    \n",
    "    # Performance metrics\n",
    "    frame_count = 0\n",
    "    total_inference_time = 0\n",
    "    \n",
    "    while st.session_state.camera_running:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            st.error(\"‚ùå Error: Failed to capture frame\")\n",
    "            break\n",
    "        \n",
    "        # Flip frame for mirror effect\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        frame_copy = frame.copy()\n",
    "        \n",
    "        # Draw ROI rectangle (GREEN - matches data collection)\n",
    "        cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right, ROI_bottom), (0, 255, 0), 3)\n",
    "        \n",
    "        # Extract and preprocess ROI with timing\n",
    "        start_time = cv2.getTickCount()\n",
    "        roi_batch, roi_display = preprocess_roi_simple(frame, ROI_top, ROI_bottom, ROI_left, ROI_right)\n",
    "        \n",
    "        # Simple hand detection (using raw ROI)\n",
    "        roi_bgr = frame[ROI_top:ROI_bottom, ROI_left:ROI_right]  # 240x240 raw BGR for hand detection\n",
    "        has_hand, hand_confidence = simple_hand_detection(roi_bgr)\n",
    "        \n",
    "        prediction_text = \"Position hand in green box\"\n",
    "        status_color = (255, 255, 255)  # White\n",
    "        \n",
    "        if has_hand and hand_confidence > 0.3:\n",
    "            # Make prediction\n",
    "            predicted_gesture, confidence, class_probs, predicted_class = create_ensemble_prediction(roi_batch)\n",
    "            \n",
    "            # Calculate inference time\n",
    "            end_time = cv2.getTickCount()\n",
    "            inference_time = (end_time - start_time) / cv2.getTickFrequency() * 1000  # ms\n",
    "            \n",
    "            # Update metrics\n",
    "            frame_count += 1\n",
    "            total_inference_time += inference_time\n",
    "            avg_inference_time = total_inference_time / frame_count\n",
    "            \n",
    "            # Determine prediction quality and display\n",
    "            if confidence >= CONFIDENCE_THRESHOLD:\n",
    "                prediction_text = f\"‚úÖ {predicted_gesture} ({confidence:.2f})\"\n",
    "                status_color = (0, 255, 0)  # Green\n",
    "                \n",
    "                # Show detailed metrics\n",
    "                metrics_info = {\n",
    "                    \"üéØ Prediction\": predicted_gesture,\n",
    "                    \"üìä Confidence\": f\"{confidence:.3f}\",\n",
    "                    \"‚ö° Inference\": f\"{inference_time:.1f}ms\",\n",
    "                    \"üìà Avg Speed\": f\"{avg_inference_time:.1f}ms\",\n",
    "                    \"üîç Hand Detection\": f\"{hand_confidence:.2f}\"\n",
    "                }\n",
    "                \n",
    "                # Show top 3 predictions for analysis\n",
    "                top3_indices = np.argsort(class_probs)[-3:][::-1]\n",
    "                top3_text = \" | \".join([f\"{word_dict[i]}:{class_probs[i]:.2f}\" for i in top3_indices])\n",
    "                \n",
    "                PREDICTION_TEXT.success(f\"**{predicted_gesture}** (Confidence: {confidence:.3f})\")\n",
    "                METRICS_TEXT.info(f\"üèÜ Top 3: {top3_text} | ‚ö° Speed: {inference_time:.1f}ms\")\n",
    "                \n",
    "            elif confidence >= 0.5:  # Medium confidence\n",
    "                prediction_text = f\"‚ö†Ô∏è {predicted_gesture}? ({confidence:.2f})\"\n",
    "                status_color = (0, 165, 255)  # Orange\n",
    "                PREDICTION_TEXT.warning(f\"Medium confidence: **{predicted_gesture}** ({confidence:.3f})\")\n",
    "                METRICS_TEXT.info(f\"‚ö° Speed: {inference_time:.1f}ms\")\n",
    "                \n",
    "            else:  # Low confidence\n",
    "                prediction_text = f\"‚ùì Unclear ({confidence:.2f})\"\n",
    "                status_color = (0, 100, 255)  # Red-orange\n",
    "                PREDICTION_TEXT.error(\"Low confidence - adjust hand position\")\n",
    "                METRICS_TEXT.info(f\"‚ö° Speed: {inference_time:.1f}ms\")\n",
    "        else:\n",
    "            # No hand detected\n",
    "            PREDICTION_TEXT.info(\"üëã Position your hand in the green box\")\n",
    "            METRICS_TEXT.info(\"üîç Analyzing...\")\n",
    "            \n",
    "        # Add prediction text to frame\n",
    "        cv2.putText(frame_copy, prediction_text, (ROI_left, ROI_top - 10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, status_color, 2)\n",
    "        \n",
    "        # Add hand detection status\n",
    "        hand_status = f\"Hand: {hand_confidence:.2f}\" if has_hand else \"No hand\"\n",
    "        cv2.putText(frame_copy, hand_status, (ROI_left, ROI_bottom + 25), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "        \n",
    "        # Add ROI info\n",
    "        roi_info = f\"ROI: {ROI_right-ROI_left}x{ROI_bottom-ROI_top}\"\n",
    "        cv2.putText(frame_copy, roi_info, (ROI_left, ROI_bottom + 50), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        \n",
    "        # Convert frame to RGB for Streamlit\n",
    "        frame_rgb = cv2.cvtColor(frame_copy, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Update display\n",
    "        FRAME_WINDOW.image(frame_rgb)\n",
    "        \n",
    "        # Break condition (Streamlit handles this)\n",
    "        if not st.session_state.camera_running:\n",
    "            break\n",
    "    \n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Final performance summary\n",
    "    if frame_count > 0:\n",
    "        st.success(f\"‚úÖ Session complete! Avg inference time: {avg_inference_time:.1f}ms ({frame_count} frames)\")\n",
    "\n",
    "# Footer information\n",
    "st.markdown(\"---\")\n",
    "st.markdown(f\"\"\"\n",
    "### üîß **System Information**\n",
    "- **Domain Match**: Training and inference use identical webcam conditions ‚úÖ\n",
    "- **ROI Coordinates**: {ROI_left},{ROI_top} to {ROI_right},{ROI_bottom} (same as data collection) ‚úÖ\n",
    "- **ROI Size**: 240√ó240 ‚Üí 224√ó224 (optimized for MobileNetV2) ‚úÖ\n",
    "- **Preprocessing**: Raw ROI ‚Üí resize ‚Üí BGR2RGB ‚Üí normalize [0,1] ‚úÖ\n",
    "- **Models**: Custom CNN + Enhanced MobileNetV2 ensemble ({model_version})\n",
    "- **Performance**: Optimized for real-time inference ‚úÖ\n",
    "\"\"\")\n",
    "\n",
    "if model_version == \"OLD\":\n",
    "    st.markdown(\"\"\"\n",
    "    ### ‚ö†Ô∏è **To Get Best Performance**\n",
    "    1. Run `ASLp_new.ipynb` to train new models on your webcam data\n",
    "    2. Make sure you have these files: `new_custom_cnn.h5` and `custom_mobilenet.h5`\n",
    "    3. Restart this app to automatically use the new models\n",
    "    4. Expect **significantly improved** accuracy and confidence!\n",
    "    \n",
    "    **üîç Current Model Files Expected:**\n",
    "    - ‚úÖ `new_custom_cnn.h5` (for new custom CNN)\n",
    "    - ‚úÖ `custom_mobilenet.h5` (for new MobileNet - note: not new_mobilenet.h5)\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b913e6-b0ff-4c1d-87ba-c6dfd27f7a84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
