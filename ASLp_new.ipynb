{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎯 **ASL GESTURE RECOGNITION - NEW WEBCAM DATASET TRAINING**\n",
    "\n",
    "**Optimized for 600 high-quality webcam images**\n",
    "- Dataset: Dataset_new/ (60 images per gesture × 10 gestures)\n",
    "- Domain: Real webcam conditions (perfect match for inference)\n",
    "- Strategy: Aggressive augmentation + Transfer learning + Strong regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Libraries imported successfully!\n",
      "🔥 TensorFlow version: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "# 📦 IMPORTS\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Flatten, Dropout, BatchNormalization, \n",
    "    Conv2D, MaxPool2D, GlobalAveragePooling2D, Input\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    ")\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"📚 Libraries imported successfully!\")\n",
    "print(f\"🔥 TensorFlow version: {tf.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 DATASET ANALYSIS:\n",
      "   Class 0: 60 images\n",
      "   Class 1: 60 images\n",
      "   Class 2: 60 images\n",
      "   Class 3: 60 images\n",
      "   Class 4: 60 images\n",
      "   Class 5: 60 images\n",
      "   Class 6: 60 images\n",
      "   Class 7: 60 images\n",
      "   Class 8: 60 images\n",
      "   Class 9: 60 images\n",
      "\n",
      "✅ Total images: 600\n",
      "📏 Target image size: 224x224\n",
      "🎯 Batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# 📁 DATASET CONFIGURATION & ANALYSIS\n",
    "DATASET_PATH = 'Dataset_new'  # New webcam dataset\n",
    "IMG_SIZE = 224  # Optimal for MobileNetV2 (upscale from 240x240 ROI)\n",
    "BATCH_SIZE = 16  # Smaller batch for limited data\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# 📊 Check dataset structure\n",
    "print(\"📊 DATASET ANALYSIS:\")\n",
    "total_images = 0\n",
    "for i in range(NUM_CLASSES):\n",
    "    class_path = os.path.join(DATASET_PATH, str(i))\n",
    "    if os.path.exists(class_path):\n",
    "        count = len([f for f in os.listdir(class_path) if f.endswith('.jpg')])\n",
    "        total_images += count\n",
    "        print(f\"   Class {i}: {count} images\")\n",
    "    else:\n",
    "        print(f\"   ❌ Class {i}: Folder missing\")\n",
    "\n",
    "print(f\"\\n✅ Total images: {total_images}\")\n",
    "print(f\"📏 Target image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"🎯 Batch size: {BATCH_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎨 Data augmentation configured:\n",
      "   ✅ Aggressive geometric transformations\n",
      "   ✅ Photometric variations\n",
      "   ✅ 80/20 train/validation split\n",
      "   ✅ No horizontal flip (preserves gesture meaning)\n"
     ]
    }
   ],
   "source": [
    "# 🎨 AGGRESSIVE DATA AUGMENTATION (Optimized for small dataset)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    # Normalization\n",
    "    rescale=1./255,\n",
    "    \n",
    "    # Geometric augmentation\n",
    "    rotation_range=25,          # Increased from 20\n",
    "    width_shift_range=0.15,     # Hand position variation\n",
    "    height_shift_range=0.15,    # Hand position variation  \n",
    "    zoom_range=0.25,            # Increased from 0.2\n",
    "    horizontal_flip=False,      # Don't flip - changes gesture meaning\n",
    "    \n",
    "    # Photometric augmentation\n",
    "    brightness_range=[0.8, 1.2],  # Lighting variation\n",
    "    channel_shift_range=20,       # Color variation\n",
    "    \n",
    "    # Advanced augmentation\n",
    "    shear_range=10,              # Slight perspective change\n",
    "    fill_mode='constant',        # Black fill (matches ROI background)\n",
    "    cval=0,                      # Black color value\n",
    "    \n",
    "    # Validation split\n",
    "    validation_split=0.2         # 80% train, 20% validation\n",
    ")\n",
    "\n",
    "# 📊 Validation data (no augmentation)\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "print(\"🎨 Data augmentation configured:\")\n",
    "print(\"   ✅ Aggressive geometric transformations\")\n",
    "print(\"   ✅ Photometric variations\")\n",
    "print(\"   ✅ 80/20 train/validation split\")\n",
    "print(\"   ✅ No horizontal flip (preserves gesture meaning)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 480 images belonging to 10 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120 images belonging to 10 classes.\n",
      "📂 Data generators created:\n",
      "   🏋️ Training samples: 480\n",
      "   ✅ Validation samples: 120\n",
      "   🎯 Classes found: 10\n",
      "   📊 Class mapping: {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n"
     ]
    }
   ],
   "source": [
    "# 📂 DATA LOADING\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical', \n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"📂 Data generators created:\")\n",
    "print(f\"   🏋️ Training samples: {train_generator.samples}\")\n",
    "print(f\"   ✅ Validation samples: {validation_generator.samples}\")\n",
    "print(f\"   🎯 Classes found: {len(train_generator.class_indices)}\")\n",
    "print(f\"   📊 Class mapping: {train_generator.class_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏗️ Improved Custom CNN created with:\n",
      "   ✅ Batch Normalization for stability\n",
      "   ✅ Heavy Dropout (0.25, 0.5) for regularization\n",
      "   ✅ Global Average Pooling vs Flatten\n",
      "   ✅ Optimized for small dataset\n"
     ]
    }
   ],
   "source": [
    "# 🏗️ IMPROVED CUSTOM CNN (Optimized for small dataset)\n",
    "def create_improved_cnn():\n",
    "    \"\"\"Enhanced CNN with strong regularization for small datasets\"\"\"\n",
    "    model = Sequential([\n",
    "        # Block 1\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        MaxPool2D(2, 2),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 2  \n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPool2D(2, 2),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 3\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(), \n",
    "        GlobalAveragePooling2D(),  # Better than Flatten for small datasets\n",
    "        \n",
    "        # Dense layers with heavy regularization\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create improved CNN\n",
    "custom_cnn = create_improved_cnn()\n",
    "print(\"🏗️ Improved Custom CNN created with:\")\n",
    "print(\"   ✅ Batch Normalization for stability\")\n",
    "print(\"   ✅ Heavy Dropout (0.25, 0.5) for regularization\") \n",
    "print(\"   ✅ Global Average Pooling vs Flatten\")\n",
    "print(\"   ✅ Optimized for small dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Enhanced MobileNetV2 created with:\n",
      "   ✅ Fine-tuning enabled (last 30 layers)\n",
      "   ✅ Custom regularized head\n",
      "   ✅ Optimized for small dataset\n"
     ]
    }
   ],
   "source": [
    "# 🚀 ENHANCED MOBILENETV2 (Fine-tuning approach)\n",
    "def create_enhanced_mobilenet():\n",
    "    \"\"\"MobileNetV2 with fine-tuning for small datasets\"\"\"\n",
    "    # Load pre-trained MobileNetV2\n",
    "    base_model = tf.keras.applications.MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    "    )\n",
    "    \n",
    "    # Unfreeze top layers for fine-tuning (better for small datasets)\n",
    "    base_model.trainable = True\n",
    "    fine_tune_at = len(base_model.layers) - 30  # Unfreeze last 30 layers\n",
    "    \n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Add custom top\n",
    "    inputs = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Create enhanced MobileNet\n",
    "mobilenet = create_enhanced_mobilenet()\n",
    "\n",
    "print(\"🚀 Enhanced MobileNetV2 created with:\")\n",
    "print(\"   ✅ Fine-tuning enabled (last 30 layers)\")\n",
    "print(\"   ✅ Custom regularized head\")\n",
    "print(\"   ✅ Optimized for small dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Models compiled with optimized settings for small dataset\n",
      "📞 Callbacks configured with strong early stopping\n",
      "\n",
      "🏋️ Training Custom CNN...\n",
      "Epoch 1/50\n",
      "30/30 [==============================] - 40s 1s/step - loss: 2.3119 - accuracy: 0.1000 - val_loss: 2.3032 - val_accuracy: 0.1000 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 35s 1s/step - loss: 2.2869 - accuracy: 0.1354 - val_loss: 2.3050 - val_accuracy: 0.1000 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 36s 1s/step - loss: 2.2529 - accuracy: 0.1792 - val_loss: 2.3061 - val_accuracy: 0.1000 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 39s 1s/step - loss: 2.1488 - accuracy: 0.1896 - val_loss: 2.3068 - val_accuracy: 0.1000 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 39s 1s/step - loss: 2.0306 - accuracy: 0.2354 - val_loss: 2.3265 - val_accuracy: 0.1000 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.9659 - accuracy: 0.1937\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 2.9999999242136255e-05.\n",
      "30/30 [==============================] - 39s 1s/step - loss: 1.9659 - accuracy: 0.1937 - val_loss: 2.3687 - val_accuracy: 0.1000 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 39s 1s/step - loss: 1.9684 - accuracy: 0.2021 - val_loss: 2.4541 - val_accuracy: 0.1000 - lr: 3.0000e-05\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 38s 1s/step - loss: 1.9186 - accuracy: 0.2146 - val_loss: 2.5025 - val_accuracy: 0.1500 - lr: 3.0000e-05\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 38s 1s/step - loss: 1.9395 - accuracy: 0.2167 - val_loss: 2.5434 - val_accuracy: 0.1000 - lr: 3.0000e-05\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 37s 1s/step - loss: 1.9137 - accuracy: 0.2354 - val_loss: 2.5351 - val_accuracy: 0.1000 - lr: 3.0000e-05\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.9020 - accuracy: 0.2438Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
      "30/30 [==============================] - 39s 1s/step - loss: 1.9020 - accuracy: 0.2438 - val_loss: 2.5953 - val_accuracy: 0.1000 - lr: 3.0000e-05\n",
      "Epoch 11: early stopping\n",
      "\n",
      "🏋️ Training Enhanced MobileNetV2...\n",
      "Epoch 1/50\n",
      "30/30 [==============================] - 20s 433ms/step - loss: 2.5617 - accuracy: 0.2062 - val_loss: 2.0149 - val_accuracy: 0.2917 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 11s 360ms/step - loss: 1.8236 - accuracy: 0.3938 - val_loss: 2.0833 - val_accuracy: 0.1500 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 11s 374ms/step - loss: 1.4610 - accuracy: 0.4708 - val_loss: 1.4096 - val_accuracy: 0.5917 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 11s 378ms/step - loss: 1.1576 - accuracy: 0.5833 - val_loss: 1.3271 - val_accuracy: 0.5917 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 12s 397ms/step - loss: 1.0034 - accuracy: 0.6229 - val_loss: 1.0138 - val_accuracy: 0.6333 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 12s 407ms/step - loss: 0.7742 - accuracy: 0.7292 - val_loss: 0.7463 - val_accuracy: 0.7167 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 11s 380ms/step - loss: 0.7416 - accuracy: 0.7333 - val_loss: 0.6979 - val_accuracy: 0.7250 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 13s 414ms/step - loss: 0.6703 - accuracy: 0.7729 - val_loss: 0.4211 - val_accuracy: 0.8667 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 12s 386ms/step - loss: 0.6050 - accuracy: 0.7833 - val_loss: 0.5225 - val_accuracy: 0.8250 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 12s 411ms/step - loss: 0.5598 - accuracy: 0.8021 - val_loss: 0.5243 - val_accuracy: 0.8167 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 12s 403ms/step - loss: 0.5324 - accuracy: 0.8042 - val_loss: 0.3411 - val_accuracy: 0.8500 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 11s 375ms/step - loss: 0.4515 - accuracy: 0.8208 - val_loss: 0.3138 - val_accuracy: 0.9167 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 13s 417ms/step - loss: 0.4826 - accuracy: 0.8438 - val_loss: 0.1809 - val_accuracy: 0.9333 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 12s 395ms/step - loss: 0.3398 - accuracy: 0.8813 - val_loss: 0.2486 - val_accuracy: 0.9417 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 11s 379ms/step - loss: 0.3676 - accuracy: 0.8792 - val_loss: 0.2955 - val_accuracy: 0.9000 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 12s 387ms/step - loss: 0.3191 - accuracy: 0.9000 - val_loss: 0.6029 - val_accuracy: 0.8417 - lr: 1.0000e-04\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 11s 364ms/step - loss: 0.3564 - accuracy: 0.8750 - val_loss: 0.9942 - val_accuracy: 0.7833 - lr: 1.0000e-04\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.2771 - accuracy: 0.8875\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 2.9999999242136255e-05.\n",
      "30/30 [==============================] - 12s 404ms/step - loss: 0.2771 - accuracy: 0.8875 - val_loss: 0.2105 - val_accuracy: 0.9250 - lr: 1.0000e-04\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 12s 383ms/step - loss: 0.2517 - accuracy: 0.9146 - val_loss: 0.1445 - val_accuracy: 0.9583 - lr: 3.0000e-05\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 12s 406ms/step - loss: 0.2676 - accuracy: 0.9021 - val_loss: 0.0935 - val_accuracy: 0.9750 - lr: 3.0000e-05\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 13s 445ms/step - loss: 0.2352 - accuracy: 0.9062 - val_loss: 0.0901 - val_accuracy: 0.9917 - lr: 3.0000e-05\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 12s 388ms/step - loss: 0.2058 - accuracy: 0.9333 - val_loss: 0.0749 - val_accuracy: 0.9917 - lr: 3.0000e-05\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 12s 387ms/step - loss: 0.2230 - accuracy: 0.9125 - val_loss: 0.0971 - val_accuracy: 0.9750 - lr: 3.0000e-05\n",
      "Epoch 24/50\n",
      "30/30 [==============================] - 12s 387ms/step - loss: 0.1815 - accuracy: 0.9333 - val_loss: 0.1455 - val_accuracy: 0.9583 - lr: 3.0000e-05\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 11s 380ms/step - loss: 0.2137 - accuracy: 0.9125 - val_loss: 0.0907 - val_accuracy: 0.9750 - lr: 3.0000e-05\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 12s 407ms/step - loss: 0.1945 - accuracy: 0.9333 - val_loss: 0.1016 - val_accuracy: 0.9750 - lr: 3.0000e-05\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.1558 - accuracy: 0.9542\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
      "30/30 [==============================] - 11s 371ms/step - loss: 0.1558 - accuracy: 0.9542 - val_loss: 0.0989 - val_accuracy: 0.9667 - lr: 3.0000e-05\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 12s 401ms/step - loss: 0.1632 - accuracy: 0.9396 - val_loss: 0.0760 - val_accuracy: 0.9833 - lr: 9.0000e-06\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 12s 407ms/step - loss: 0.1646 - accuracy: 0.9396 - val_loss: 0.0739 - val_accuracy: 0.9917 - lr: 9.0000e-06\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 12s 397ms/step - loss: 0.1306 - accuracy: 0.9563 - val_loss: 0.0813 - val_accuracy: 0.9917 - lr: 9.0000e-06\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 13s 419ms/step - loss: 0.1655 - accuracy: 0.9417 - val_loss: 0.0790 - val_accuracy: 0.9833 - lr: 9.0000e-06\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 12s 406ms/step - loss: 0.1915 - accuracy: 0.9396 - val_loss: 0.0781 - val_accuracy: 0.9917 - lr: 9.0000e-06\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 12s 412ms/step - loss: 0.1639 - accuracy: 0.9375 - val_loss: 0.0864 - val_accuracy: 0.9750 - lr: 9.0000e-06\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 12s 393ms/step - loss: 0.1543 - accuracy: 0.9479 - val_loss: 0.0724 - val_accuracy: 0.9917 - lr: 9.0000e-06\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 12s 383ms/step - loss: 0.1867 - accuracy: 0.9396 - val_loss: 0.0703 - val_accuracy: 0.9833 - lr: 9.0000e-06\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 12s 405ms/step - loss: 0.1420 - accuracy: 0.9563 - val_loss: 0.0714 - val_accuracy: 0.9833 - lr: 9.0000e-06\n",
      "Epoch 37/50\n",
      "30/30 [==============================] - 12s 382ms/step - loss: 0.1314 - accuracy: 0.9542 - val_loss: 0.0678 - val_accuracy: 0.9833 - lr: 9.0000e-06\n",
      "Epoch 38/50\n",
      "30/30 [==============================] - 13s 415ms/step - loss: 0.1394 - accuracy: 0.9458 - val_loss: 0.0719 - val_accuracy: 0.9750 - lr: 9.0000e-06\n",
      "Epoch 39/50\n",
      "30/30 [==============================] - 12s 396ms/step - loss: 0.1734 - accuracy: 0.9500 - val_loss: 0.0734 - val_accuracy: 0.9833 - lr: 9.0000e-06\n",
      "Epoch 40/50\n",
      "30/30 [==============================] - 12s 403ms/step - loss: 0.1477 - accuracy: 0.9375 - val_loss: 0.0667 - val_accuracy: 0.9833 - lr: 9.0000e-06\n",
      "Epoch 41/50\n",
      "30/30 [==============================] - 12s 411ms/step - loss: 0.1465 - accuracy: 0.9563 - val_loss: 0.0677 - val_accuracy: 0.9833 - lr: 9.0000e-06\n",
      "Epoch 42/50\n",
      "30/30 [==============================] - 12s 399ms/step - loss: 0.1847 - accuracy: 0.9333 - val_loss: 0.0676 - val_accuracy: 0.9833 - lr: 9.0000e-06\n",
      "Epoch 43/50\n",
      "30/30 [==============================] - 13s 430ms/step - loss: 0.1537 - accuracy: 0.9458 - val_loss: 0.0755 - val_accuracy: 0.9833 - lr: 9.0000e-06\n",
      "Epoch 44/50\n",
      "30/30 [==============================] - 13s 410ms/step - loss: 0.1694 - accuracy: 0.9333 - val_loss: 0.0751 - val_accuracy: 0.9750 - lr: 9.0000e-06\n",
      "Epoch 45/50\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.1281 - accuracy: 0.9583\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
      "30/30 [==============================] - 14s 450ms/step - loss: 0.1281 - accuracy: 0.9583 - val_loss: 0.0792 - val_accuracy: 0.9833 - lr: 9.0000e-06\n",
      "Epoch 46/50\n",
      "30/30 [==============================] - 14s 447ms/step - loss: 0.2170 - accuracy: 0.9292 - val_loss: 0.0803 - val_accuracy: 0.9833 - lr: 2.7000e-06\n",
      "Epoch 47/50\n",
      "30/30 [==============================] - 11s 377ms/step - loss: 0.1316 - accuracy: 0.9583 - val_loss: 0.0750 - val_accuracy: 0.9833 - lr: 2.7000e-06\n",
      "Epoch 48/50\n",
      "30/30 [==============================] - 13s 425ms/step - loss: 0.1271 - accuracy: 0.9396 - val_loss: 0.0675 - val_accuracy: 0.9833 - lr: 2.7000e-06\n",
      "Epoch 49/50\n",
      "30/30 [==============================] - 11s 369ms/step - loss: 0.1303 - accuracy: 0.9521 - val_loss: 0.0659 - val_accuracy: 0.9833 - lr: 2.7000e-06\n",
      "Epoch 50/50\n",
      "30/30 [==============================] - 12s 395ms/step - loss: 0.0966 - accuracy: 0.9750 - val_loss: 0.0629 - val_accuracy: 0.9833 - lr: 2.7000e-06\n",
      "\n",
      "✅ Training completed!\n"
     ]
    }
   ],
   "source": [
    "# ⚙️ COMPILATION & TRAINING\n",
    "\n",
    "# Create an optimizer for the Custom CNN\n",
    "optimizer_cnn = Adam(learning_rate=0.0001, weight_decay=1e-4)\n",
    "\n",
    "# Create a separate, new optimizer for MobileNetV2\n",
    "optimizer_mobilenet = Adam(learning_rate=0.0001, weight_decay=1e-4)\n",
    "\n",
    "# Compile the models with their respective optimizers\n",
    "custom_cnn.compile(optimizer=optimizer_cnn, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "mobilenet.compile(optimizer=optimizer_mobilenet, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks can be shared\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=1e-7, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"⚙️ Models compiled with optimized settings for small dataset\")\n",
    "print(\"📞 Callbacks configured with strong early stopping\")\n",
    "\n",
    "# 🏋️ TRAIN MODELS\n",
    "print(\"\\n🏋️ Training Custom CNN...\")\n",
    "history_cnn = custom_cnn.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n🏋️ Training Enhanced MobileNetV2...\")\n",
    "history_mobilenet = mobilenet.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎭 ENSEMBLE MODEL Results:\n",
      "   📉 Validation Loss: 0.6378\n",
      "   🎯 Validation Accuracy: 0.9833 (98.33%)\n",
      "\n",
      "💾 Saving trained models...\n",
      "✅ Models saved:\n",
      "   📁 new_custom_cnn.h5\n",
      "   📁 new_mobilenet.h5\n",
      "   🎭 Ensemble: Will be recreated in app.py\n",
      "\n",
      "🎉 TRAINING COMPLETE!\n",
      "📊 Best performance: 98.33%\n",
      "🚀 Ready for deployment!\n"
     ]
    }
   ],
   "source": [
    "# 🎭 Create and evaluate ensemble\n",
    "inputs = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "pred1 = custom_cnn(inputs)\n",
    "pred2 = mobilenet(inputs)\n",
    "ensemble_output = tf.keras.layers.Average()([pred1, pred2])\n",
    "ensemble_model = Model(inputs=inputs, outputs=ensemble_output)\n",
    "\n",
    "# Compile with explicit optimizer\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, weight_decay=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_scores = ensemble_model.evaluate(validation_generator, verbose=0)\n",
    "print(f\"\\n🎭 ENSEMBLE MODEL Results:\")\n",
    "print(f\"   📉 Validation Loss: {ensemble_scores[0]:.4f}\")\n",
    "print(f\"   🎯 Validation Accuracy: {ensemble_scores[1]:.4f} ({ensemble_scores[1]*100:.2f}%)\")\n",
    "\n",
    "# 💾 Saving trained models...\n",
    "print(f\"\\n💾 Saving trained models...\")\n",
    "custom_cnn.save('new_custom_cnn.h5')\n",
    "mobilenet.save('new_mobilenet.h5')\n",
    "\n",
    "print(f\"✅ Models saved:\")\n",
    "print(f\"   📁 new_custom_cnn.h5\")\n",
    "print(f\"   📁 new_mobilenet.h5\")\n",
    "print(f\"   🎭 Ensemble: Will be recreated in app.py\")\n",
    "\n",
    "print(f\"\\n🎉 TRAINING COMPLETE!\")\n",
    "print(f\"📊 Best performance: {ensemble_scores[1]*100:.2f}%\")\n",
    "print(f\"🚀 Ready for deployment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
